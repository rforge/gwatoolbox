\documentclass[a4paper, 10pt]{article}
\usepackage{graphicx}
\usepackage{makeidx}
\usepackage{latexsym}
\usepackage[pdftex,colorlinks]{hyperref}
% \usepackage[utf8]{inputenc}

\title{GWAtoolbox \\An R package for the fast processing of data from Genome-Wide Association Studies}
\author{Christian Fuchsberger \and Daniel Taliun \and Cristian Pattaro}

% \VignetteIndexEntry{GWAtoolbox}

\newcounter{example_cnt}

\newenvironment{example}{\refstepcounter{example_cnt} \begin{quotation} \noindent \textbf{Example \arabic{example_cnt}} \sf}{$\lhd$ \end{quotation}}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\makeindex

\begin{document}
\maketitle
\newpage

\tableofcontents
\newpage

\section{Introduction}

\textbf{\emph{GWAtoolbox}} is an \emph{R} package for processing data originated from Genome-Wide Association Studies (GWAS). 
GWAS have become increasingly popular in the last years, leading to the discovery of hundreds of common genetic variants affecting the risk of diseases (such as diabetes, hypertension, chronic kidney disease, etc.) or the level of quantitative biological parameters.

Results from GWAS typically consist in large files where, for each single nucleotide polymorphisms (SNP), statistics related to the association between the SNP and the studied trait are stored. 
The number of SNPs which is currently being analyzed in most GWAS is in excess of 2.5 Million and is expected to increase rapidly. 
For each individual SNP, the minimal information stored consists of the SNP identification code (SNPID), chromosomal position, genotype (reference and non-reference alleles), frequency of the reference allele, and SNP effect size and its standard error. Additional information such as p-value, minor allele frequency (MAF), and an imputation quality index are often provided.
As a consequence, the typical dimension of GWAS result files is of >2.5 Million rows by >9 columns, for a total file size which is often larger than 300 Mbytes.

With the aim of detecting common or less common genetic variants with modest effects, it is now common practice to pool results from individual studies into meta-analysis efforts which not rarely involve dozens of studies. 
In these consortia initiatives, each individual study contributes several files either because multiple traits are being analyzed or because different analyses on the same trait are needed.
Consequently, statisticians working in consortia have to deal with a massive amount of files which need to be quality controlled to avoid problems during the meta-analysis process. 
As a result of the quality control (QC) process, some files could be found to be corrupted or erroneous so that new data upload is needed from individuals studies. 
In this way, the loop between the consortium and the individual study analyst originates multiple file checks, until a satisfactory data quality is achieved.

When working with such large datasets in R, simple operations such as the uploading files into the R working space, file management, and data plotting, can take considerable time, and a systematic QC of hundreds of files can be unfeasible or may require several weeks.

With the \emph{GWAtoolbox} we provide a set of instruments to simplify the data handling in the framework of meta-analyses of GWA data.
The function \emph{gwasqc()} is capable to process a high number of GWAS data files in a single run, and producing several QC reports and figures. A routine for the between-study comparison is also provided to check systematic difference between files. In addition, the package contains annotation and graphical tools to help the result interpretation.

\section{Installation}
\emph{GWAtoolbox} package can be downloaded from \url{http://www.eurac.edu/GWAtoolbox.html}.
It requires R version 2.9.2 or higher. The installation of package varies depending on your host operating system and user privileges. In this section we provide detailed installation instructions for a wide range of settings.

\subsection{Windows}

\emph{GWAtoolbox} for Windows is distributed in compiled binary form.
The following steps describe the installation procedure:

\begin{itemize}
\item[1. ] Download the latest package version \emph{GWAtoolbox\_X.Y.Z.zip}.
\item[2. ] Start the R program.
\item[3a.] If you have administrator privileges (you can install packages to the main R library):
	\begin{itemize}
	\item[i.] Execute the command:
		\begin{quotation}
		\noindent \texttt{install.package("path/to/GWAtoolbox\_X.Y.Z.zip", repos=NULL)}
		\end{quotation}
		where \texttt{path/to} is the directory of the downloaded package.
	\item[ii.] Now you can load the package in R with the command:
		\begin{quotation}
		\noindent \texttt{library(GWAtoolbox)}
		\end{quotation}
	\end{itemize}
\item[3b.] If you do NOT have sufficient privileges to install packages to the main R library directory:
	\begin{itemize}
	\item[i.] Execute the command:
		\begin{quotation}
		\noindent \texttt{install.package("path/GWAtoolbox\_X.Y.Z.zip",\linebreak lib="path/to/install/directory",\linebreak repos=NULL)}
		\end{quotation}
		where \texttt{path/to/install/directory} is the path with your install directory.
	\item[ii.] Now you can load the package in R with command:
		\begin{quotation}
		\noindent \texttt{library(GWAtoolbox, lib.loc = "path/to/install/directory")}
		\end{quotation}
	\end{itemize}
\end{itemize}

\subsection{Unix}

\emph{GWAtoolbox} for Unix is distributed in source form and, therefore, it is compiled on the user machine. 
This requires the following tools to be installed:
\begin{itemize}
\item C/C++ compilers
\item GNU Scientific Library (GSL)\footnote{http://www.gnu.org/software/gsl/} version 1.8
\end{itemize}
When these requirements are fulfilled, the following steps will guide you through the package installation process:
\begin{itemize}
\item[1. ] Download the latest package version \emph{GWAtoolbox\_X.Y.Z.tar.gz}.
\item[2a.] If you have administrator privileges (you can install packages to the main R library):
	\begin{itemize}
	\item[i.] In the Unix shell execute the command:
		\begin{quotation}
		\noindent \texttt{R CMD INSTALL path/to/GWAtoolbox\_X.Y.Z.tar.gz}
		\end{quotation}
		where \texttt{path/to} is the directory of the downloaded package.
	\item[ii.] Now you can start the R program and load the package with the command:
		\begin{quotation}
		\noindent \texttt{library(GWAtoolbox)}
		\end{quotation}
	\end{itemize}
\item[2b.] If you do NOT have sufficient privileges to install packages to the main R library directory:
	\begin{itemize}
	\item[i.] In the Unix shell execute the single line command:
		\begin{quotation}
		\noindent \texttt{R CMD INSTALL path/to/GWAtoolbox\_X.Y.Z.tar.gz \linebreak -l path/to/install/directory}
		\end{quotation}
		where \texttt{path/to/install/directory} is the path with your install directory.
	\item[ii.] Now you can start the R program and load the package with the command:
		\begin{quotation}
		\noindent \texttt{library(GWAtoolbox, lib.loc="path/to/install/directory")}
		\end{quotation}
	\end{itemize}
\end{itemize}

\section{The Quality Control Workflow}

A careful and thorough data QC should be performed before starting any meta-analysis of GWAS data, especially when many studies are involved.
In this framework, we identified three objectives of a good QC analysis:
\begin{enumerate}
\item formal checking: whether all files that will be entered in the meta-analysis process fulfill the format guidelines.
This includes:
\begin{itemize}
\item consistency of column names with meta-analysis guidelines;
\item presence of the minimal required information; 
\item the number of chromosomes is as expected; 
\item data are in a format that can be analyzed (numeric, character, factor); 
\item all SNP identification numbers are unique; 
\item alleles are coded in letters/numbers as expected; 
\item missing values are coded in a consistent way; 
\item the field separator is as expected;
\item strand assessment;
\end{itemize}
\item quality checking: evaluating the quality of data in each single file.
This includes:
\begin{itemize}
\item presence of unexpected values for some of the items required for the meta-analysis (e.g.: negative p-values or standard errors);
\item p-value inflation and p-value distribution;
\end{itemize}
\item global checking: identification of any systematic biases that can disturb the analysis. It is aimed to uncover studies that are systematically different from the others. This may happen when, for instance, analysts of one study forget to log-transform the phenotype or apply the wrong model to the data.
\end{enumerate}

Formal checks and quality checks of individual studies are performed in \emph{GWAtoolbox} using the \emph{gwasqc()} function.
% Global checking is performed partially with \emph{gwasqc()} and more thoroughly with the \emph{NAME OF FUNCTION PLEASE PROVIDE()} function.
%In particular, \emph{gwasqc()} is the core-function of the \emph{GWAtoolbox} package. 
\emph{gwasqc()} was built to include the following features:
\begin{enumerate}
\item rapid file processing and reporting;
\item eliminate routine user operations;
\item multi-format reporting which includes \emph{HTML}, \emph{CSV}, and text files.

\end{enumerate}

The complete QC workflow can be summarized in four basic steps (see Figure \ref{fig:workflow}):

\begin{enumerate}
\item collect the GWAS data files;
\item write an input script to process of all GWAS files with the \emph{gwasqc()} function;
\item run the QC using \emph{gwasqc()}; % first and \emph{NAME OF FUNCTION PLEASE PROVIDE()} then;
\item analyze the QC results to uncover errors or inconsistencies.
\end{enumerate}
\begin{figure}[!ht]
\centering
\includegraphics[type=jpg, ext=.jpg, read=.jpg, width=0.7\linewidth]{workflow}
\caption{The quality control workflow.}
\label{fig:workflow}
\end{figure}

In the next sections we cover each of the four steps and describe the requirements for the input files and the precise content of all output files.

\section{GWAS data files}
GWAS data are usually stored as delimited text files.
The first line of the file is the header row that describes the content of every column.
The field separator for the columns can be any among \emph{whitespace}, \emph{tabulation}, \emph{comma}, or \emph{semicolon}. 
The field separator must be the same for every row in the file, including the header.

There is a minimum set of columns, that every GWAS data file should contain. 
In \emph{GWAtoolbox}, the following information is required for every file:
\begin{itemize}
\item Marker name
\item Chromosome number or name
\item Marker position
\item Coded and non-coded allele
\item Allele frequency for coded allele
\item Strand
\item Imputation label
\item Imputation quality
\item Effect size
\item Standard error
\item P-value
\end{itemize}

The \emph{gwasqc()} function will take full responsibility for checking if an input file contains all the information and will report about missing data.

More non-mandatory items can be included in the data file as, for example, the study sample size, the SNP call rate for genotyped SNPs, the p-value of the Hardy-Weinberg equilibrium test for genotyped SNPs, etc.


\section{The Input Script}
\emph{gwasqc()} can analyze several GWAS data files consecutively. 
Instructions are provided using a script in a text file.
The format of the script file resembles the one of the METAL input files\footnote{http://www.sph.umich.edu/csg/abecasis/metal/}.

Within the input script file, the user can list all GWAS file names to be analyzed and specify the format of each single GWAS file, including column names, field separator, etc. 
In the case that more GWAS files are in the same format, file specifications can be entered only once, before listing the file names.
Example \ref{ex:full_script} illustrates the content of a hypothetical input script file.

\begin{example} 
\label{ex:full_script}
\begin{verbatim}
# Description of input data columns
MARKER        SNPID
CHR           Chromosome
POSITION      Position
N             n_total
ALLELE        coded_allele noncoded_allele
STRAND        strand
EFFECT        beta
STDERR        se
PVALUE        pval
FREQLABEL     allele_freq_coded_allele
IMPUTED       imputed
IMP_QUALITY   oevar_imp

# High quality filters
HQ_SNP   0.01   0.3

# Plotting filters
MAF      0.01   0.05
IMP      0.3    0.6

# Prefix for output files
PREFIX   res_

# Input file with GWA data
PROCESS  input_file.txt
\end{verbatim}
\end{example}

\subsection{Specifying Input Data Files}
The names of the GWAS data files are specified in the input script with the command \textbf{PROCESS}\index{PROCESS}\footnote{\emph{GWAtoolbox} supports single line feed ('\textbackslash n') character or carriage return character ('\textbackslash r') followed by line feed character as the line terminators in the input files.}
If multiple files have to be checked, multiple \textbf{PROCESS}\index{PROCESS} lines must be specified.

\begin{example}
The input script contains the following two lines:
\begin{verbatim}
PROCESS   input_file_1.txt
PROCESS   /dir_1/dir_2/input_file_2.csv
\end{verbatim}
Then, QC is applied first to \emph{input\_file\_1.txt} and then to \emph{input\_file\_2.csv}.
As used in the example, when files reside in different directories, the full path must be specified.
\end{example}

\subsection{Describing Input Data Columns}
\subsubsection{Field Separator}
The field separator may be different for each GWAS data file. 
The \emph{gwasqc()} function automatically detects the separator field for each input file \emph{based on the first 10 rows}. 
However, the user has the possibility to specify the separator manually for each individual file using the command \textbf{SEPARATOR}\index{SEPARATOR}. 
Table \ref{table:separator_cmd} lists all supported separators.
\begin{table}[!ht]
\begin{center}
\begin{tabular}{|l|l|}
\hline
Argument & Separator \\
\hline
COMMA\index{COMMA} & \emph{comma}\\
TAB\index{TAB} & \emph{tabulation}\\
WHITESPACE\index{WHITESPACE} & \emph{whitespace}\\
SEMICOLON\index{SEMICOLON} & \emph{semicolon}\\
\hline
\end{tabular}
\caption{The list of arguments for the SEPARATOR command.}
\label{table:separator_cmd}
\end{center}
\end{table}

\begin{example}
In the following input script:
\begin{verbatim}
PROCESS      input_file_1.txt 
SEPARATOR    TAB              
PROCESS      input_file_2.csv 
PROCESS      input_file_3.txt 
\end{verbatim}
the field separator for the input file \emph{input\_file\_1.txt} is determined automatically by \emph{gwasqc()}, but for the input files \emph{input\_file\_2.csv} and \emph{input\_file\_3.txt} the separator is manually set to tabulation.
\end{example}

\subsubsection{Missing Values}
By default \emph{gwasqc()} assumes that missing values are labeled as \emph{NA}. However, the label for missing value can be specified manually by the user with the command \textbf{MISSING}\index{MISSING}.

\begin{example}
Let's assume the following input script:
\begin{verbatim}
MISSING     -                
PROCESS     input_file_1.txt 
MISSING     NA               
PROCESS     input_file_2.csv 
\end{verbatim}
For the file \emph{input\_file\_1.txt} the \emph{hyphen} symbol is set as symbol for missing value. Afterwards, it is changed to \emph{NA} and is used to process \emph{input\_file\_2.csv}.
\end{example}

\subsubsection{Column Names}
\label{section:column_names}
In table \ref{table:default_columns} the complete list of the default column names for a GWAS data file is reported. 
These names identify uniquely the items in the GWAS data file.
\begin{table}[!ht]
\begin{center}
\begin{tabular}{|l|p{8cm}|}
\hline
Default column name(s) & Description \\
\hline
MARKER\index{MARKER} & Marker name \\
CHR\index{CHR} & Chromosome number or name \\
POSITION\index{POSITION} & Marker position \\
ALLELE1\index{ALLELE1}, ALLELE2\index{ALLELE2} & Coded and non-coded alleles \\
FREQLABEL\index{FREQLABEL} & Allele frequency for the coded allele \\
STRAND\index{STRAND} & Strand \\
IMPUTED\index{IMPUTED} & Label value indicating if the marker was imputed (1) or genotyped (0) \\
IMP\_QUALITY\index{IMP\_QUALITY} & Imputation quality statistics; this can be different depending on the software used for imputation: MACH's \emph{Rsq}, IMPUTE's \emph{properinfo}, ...  \\
EFFECT\index{EFFECT} & Effect size \\
STDERR\index{STDERR} & Standard error \\
PVALUE\index{PVALUE} & P-value \\
HWE\_PVAL\index{HWE\_PVAL} & Hardy-Weinberg equilibrium p-value \\
CALLRATE\index{CALLRATE} & Genotype callrate \\
N\index{N} & Sample size \\
USED\_FOR\_IMP\index{USED\_FOR\_IMP} & Label value indicating if a marker was used for imputation (1) or not (0) \\
\hline
\end{tabular}
\caption{The default column names.}
\label{table:default_columns}
\end{center}
\end{table}

Given that different names can be provided with the GWAS data files, \emph{gwasqc()} allows to redefine the default values for every input file in the input script. 
The redefinition command consists of the default column name followed by a new column name. To redefine the default column names for \emph{coded} and
\emph{non-coded} alleles, the command \textbf{ALLELE}\index{ALLELE} is followed by two new column names.

\begin{example}
Let's assume to have two input files, \emph{input\_file\_1.txt} and \emph{input\_file\_2.txt}. In \emph{input\_file\_1.txt}, the column names for effect size and standard error are \emph{beta} and \emph{SE}, respectively. In the \emph{input\_file\_2.txt}, the column name for the effect size is the same as in \emph{input\_file\_1.txt}, but the column name for the standard error is \emph{STDERR}. 
The correct column redefinitions are as follows:
\begin{verbatim}
EFFECT     beta
STDERR     SE
PROCESS    input_file_1.txt
STDERR     STDERR
PROCESS    input_file_2.csv
\end{verbatim}
First, we redefine column names for the input file \emph{input\_file\_1.txt}. We note that 
the column \emph{beta} doesn't need to be redefined for the input file \emph{input\_file\_2.csv}. 
However, for this file we need to redefine the column \emph{STDERR}, returning it to the default column naming.
\end{example}

\begin{example}
Consider an input file \emph{input\_file\_1.txt} with the following names for ALLELE1 and ALLELE2: \emph{myRefAllele} and \emph{myNonRefAllele}.
The new column definition is applied as follows:
\begin{verbatim}
ALLELE    myRefAllele myNonRefAllele
PROCESS   input_file_1.txt
\end{verbatim}
\end{example}

\subsubsection{Case Sensitivity}
By default the \emph{gwasqc()} function assumes that column names in the input files are case insensitive. For example, the column names \emph{STDERR}, \emph{StdErr}, and \emph{STDErr} are all perfectly equivalent.
This behaviour can be changed for every input file in the input script using the command \textbf{CASESENSITIVE}\index{CASESENSITIVE}, that controls case sensitivity for the column names. Table \ref{table:casesensitive_cmd} lists all possible arguments.
\begin{table}[!ht]
\begin{center}
\begin{tabular}{|l|l|}
\hline
Argument & Description \\
\hline
0 & Column names in the input file are case insensitive (default) \\
1 & Column names in the input file are case sensitive \\
\hline
\end{tabular}
\caption{The list of arguments for CASESENSITIVE command.}
\label{table:casesensitive_cmd}
\end{center}
\end{table}

\begin{example}
Consider the following commands:
\begin{verbatim}
CASESENSITIVE   1
PROCESS         input_file_1.txt
CASESENSITIVE   0
PROCESS         input_file_2.csv 
\end{verbatim}
In this case, the column names in the input file \emph{input\_file\_1.txt} are case sensitive and must correspond exactly to the default column names, while the column names in the input file \emph{input\_file\_2.csv} are case insensitive.
\end{example}

\subsection{Specifying Data Filters}

\subsubsection{Implausible Values Filter}
Often, there is the necessity to identify implausible values for the statistics that will be included in the meta-analysis. 
Implausible values for the effect estimate, for its standard error, and for the p-value are sometimes generated by the software used for the association testing.
In case of small numbers, which is typical of a disease outcome with a small number of cases or of a SNP with very small minor allele frequency, statistical packages can report inconsistent results.
This is due to statistical algorithms that fail to converge because of data sparseness. Other types of inconsistencies can originate from errors in the file management. \\

In these situations, it is important to identify the SNPs with inconsistent values, so that they can be removed before starting the meta-analysis.
\emph{gwasqc()} can identify these values by using appropriate threshold values. The number of SNPs affected by this kind of problems is reported. 
In addition, these SNPs are excluded from the calculation of the summary statistics on data quality.\\
The implausible values filter is used in the \emph{gwasqc()} function to identify implausible data values. Table \ref{table:implausible_values} lists the columns for which the filter is applied and the default thresholds.
\begin{table}[!ht]
\begin{center}
\begin{tabular}{|l|l|}
\hline
Default column name & Default thresholds \\
\hline
STDERR\index{STDERR} & $[0, 100000]$ \\
IMP\_QUALITY\index{IMP\_QUALITY} & $(0, 1.5)$ \\
PVALUE\index{PVALUE} & $(0, 1)$ \\
FREQLABEL\index{FREQLABEL} & $(0, 1)$ \\
HWE\_PVAL\index{HWE\_PVAL} & $(0, 1)$ \\
CALLRATE\index{CALLRATE}& $(0, 1)$ \\
\hline
\end{tabular}
\caption{The default implausible values filter.}
\label{table:implausible_values}
\end{center}
\end{table}

The default thresholds can be redefined for every column in the input script. 
The new threshold values for a column can be specified after the redefinition of the column name (see Section \ref{section:column_names}).

\begin{example}
Let's assume that the input file \emph{input\_file\_1.txt} has a standard error column called \emph{STDERR} and 
that the corresponding column in the input file \emph{input\_file\_2.csv} is called \emph{SE}. In addition, the imputation quality column is defined as \emph{oevar\_imp} in both files.
The following script shows how the user can redefine the column names while applying different plausibility filters:
\begin{verbatim}
STDERR        STDERR 0 80000
IMP_QUALITY   oevar_imp 0 1
PROCESS       input_file_1.txt
STDERR        SE 0 100000
PROCESS       input_file_2.csv
\end{verbatim}
The file \emph{input\_file\_1.txt} has new $[0, 80000]$ thresholds for the standard error column and new $(0, 1)$ threshold for the imputation quality. 
For the file \emph{input\_file\_2.csv} the thresholds of $[0, 100000]$ will be applied to the standard error column, while
for the imputation quality column the same filters as for the \emph{input\_file\_1.txt} will be applied.
\end{example}

\subsubsection{High Quality Filters}
\label{section:hq_data_filter}
In many cases, the analysis is restricted to SNPs with high imputation quality and with not too small minor allele frequency.
We call these SNPs '\emph{high quality SNPs}', that is SNPs for which results should be quite robust.
In the special case, when estimating the inflation factor, \emph{lambda}, to check the presence of cryptic relatedness or hidden population sub-structures, it can be important to remove SNPs that could artificially increase the value of \emph{lambda}. 
Summary statistics are calculated after excluding SNPs with low quality (\emph{CSV} report files). 
Table \ref{table:hq_values} lists the default thresholds for the allele frequency and for the imputation quality.
\begin{table}[!ht]
\begin{center}
\begin{tabular}{|l|l|}
\hline
Default column name & Default thresholds \\
\hline
FREQLABEL\index{FREQLABEL} & $> 0.01$ \\
IMP\_QUALITY\index{IMP\_QUALITY} & $> 0.3$ \\
\hline
\end{tabular}
\caption{The default high quality imputation filters.}
\label{table:hq_values}
\end{center}
\end{table}

The default values can be redefined using the command \textbf{HQ\_SNP}\index{HQ\_SNP} for every input file in the input script. 
The command is followed by two values: the first one corresponds to the threshold for the minor allele frequency, 
and the second one corresponds to the threshold for the imputation quality.

\begin{example}
If we want to define 'high quality SNPs' those with minor allele frequency $> 0.03$ and with imputation quality $> 0.4$, we would add the following lines to the input script:
\begin{verbatim}
HQ_SNP     0.03 0.4
PROCESS    input_file_1.txt
\end{verbatim}
\end{example}

\subsubsection{Plotting Filter}
\label{section:plotting_filter}
The plotting filter is used to select appropriate data for the QQ-plots, boxplots and histograms. The filter has two threshold levels: each of them is applied dependently on the plot type and column. Figure \ref{fig:plots} (see Section \ref{section:number_and_content_of_plots}) shows what data and filters are used when producing plots. Table \ref{table:maf_imp_values} lists the default threshold values.
\begin{table}[!ht]
\begin{center}
\begin{tabular}{|l|p{3cm}|p{3cm}|}
\hline
Default column name & Default 1st level thresholds & Default 2nd level thresholds\\
\hline
FREQLABEL\index{FREQLABEL} & $> 0.01$ & $> 0.05$ \\
IMP\_QUALITY\index{IMP\_QUALITY} & $> 0.3$ & $> 0.6$ \\
\hline
\end{tabular}
\caption{The default plotting filter.}
\label{table:maf_imp_values}
\end{center}
\end{table}

The default threshold values for the coded allele frequency and imputation quality can be redefined accordingly with the commands \textbf{MAF}\index{MAF} and \textbf{IMP}\index{IMP} for the every input file in the input script.

\begin{example}
Assume the input script contains the following commands:
\begin{verbatim}
MAF        0.02 0.03
IMP        0.3 0.5
PROCESS    input_file_1.txt
\end{verbatim}
In this example new plotting filter thresholds are set for the input file \emph{input\_file\_1.txt}. For the first level threshold the coded allele frequency $> 0.02$ and the imputation quality $> 0.3$, while for the second level threshold the coded allele frequency $> 0.03$ and imputation quality $> 0.5$.
\end{example}

\subsection{Specifying Output Files}

\subsubsection{Output File Name}
The output file names are constructed from the input file names by adding the specified prefix. This is done both for the textual output files and image files. The prefix can be specified once for all input files, or for every single input file or groups of input files explicitly using the command \textbf{PREFIX}\index{PREFIX}.

\begin{example}
Consider the following input script:
\begin{verbatim}
PREFIX       res_
PROCESS      input_file_1.txt
PROCESS      input_file_2.csv

PREFIX       result_
PROCESS      input_file_3.tab
\end{verbatim}
In this example, all the result output files corresponding to the input files \emph{input\_file\_1.txt} and \emph{input\_file\_2.csv} will be prefixed with \emph{res\_}, while the result output files corresponding to the input file \emph{input\_file\_3.tab} will be prefixed with \emph{result\_}.
\end{example}

\subsubsection{Verbosity Level}
The \emph{GWAtoolbox} package provides the possibility to control the number of generated output figures using command \textbf{VERBOSITY}\index{VERBOSITY} (see Table \ref{table:verbosity_cmd} for the available options). 
\begin{table}[!ht]
\begin{center}
\begin{tabular}{|l|p{8cm}|}
\hline
Argument & Description \\
\hline
1 & The default and the lowest verbosity level. \\
2 & The highest verbosity level. \\
\hline
\end{tabular}
\caption{The list of arguments for the VERBOSITY command.}
\label{table:verbosity_cmd}
\end{center}
\end{table}

\begin{example}
Assume the input script contains the following commands:
\begin{verbatim}
VERBOSITY       2
PROCESS         input_file_1.txt
VERBOSITY       1
PROCESS         input_file_2.csv 
\end{verbatim}
In this example the input file \emph{input\_file\_1.txt} is processed with the highest verbosity level and therefore all figures are produced, while the input file \emph{input\_file\_2.csv} is processed with the lowest verbosity level and less output figures are generated.
\end{example}

\subsubsection{Number And Content Of Plots}
\label{section:number_and_content_of_plots}
Number and content of the output plots depend on the setting of the plotting filters (see Section \ref{section:plotting_filter}) and on the available columns in the input file. Figure \ref{fig:plots} shows the dependencies. If some dependency is not satisfied because of missing column or filter setting, then the corresponding plot is not produced or may be truncated at different levels.
\begin{figure}[!ht]
\centering
\includegraphics[type=jpg, ext=.jpg, read=.jpg, width=0.5\linewidth]{plots}
\caption{The dependency of plots on columns and filters.}
\label{fig:plots}
\end{figure}

Furthermore, the boxplots comparing the \emph{EFFECT}\index{EFFECT} distributions across studies allow the specification of a \textbf{BOXPLOTWIDTH}\index{BOXPLOTWIDTH} that can be based on one of the other available information (typically the sample size).
As an argument, \textbf{BOXPLOTWIDTH}\index{BOXPLOTWIDTH} requires one of the default column names.
If \textbf{BOXPLOTWIDTH}\index{BOXPLOTWIDTH} is not specified all boxplots have the same width.

It is also possible to specify labels for every input file, to be used in the plots instead of the full file names, which could be too long and, therefore, clutter the plots.

\begin{example}
Let \emph{n\_total} be the column name which identifies the sample size in the input file \emph{input\_file\_1.txt}, and \emph{samplesize} the corresponding name in \emph{input\_file\_2.csv}.
Then, consider the following input script:
\begin{verbatim}
N              n_total
PROCESS        input_file_1.txt	first
N              samplesize
PROCESS        /dir_1/dir_2/input_file_2.csv	second
BOXPLOTWIDTH   N
\end{verbatim}
In this example, the width of the first boxplot for the input file \emph{input\_file\_1.txt} depends on the \emph{n\_total} column, while the width of the second boxplot for the input file \emph{input\_file\_2.csv} depends on the \emph{samplesize} column.
The labels "first" and "second" will be used to label the two studies in the plots.
\end{example}

\section{The Output Files}
The \emph{GWAtoolbox} package produces four types of files:
\begin{enumerate}
\item Figures with QQ-plots, histograms and boxplots (see Figure \ref{fig:plots} for all employed input columns and filters).
\item Textual report file with \emph{.txt} extension.
\item Comma separated file with \emph{.csv} extension, that contains statistics for the high quality imputation data (see Section \ref{section:hq_data_filter}).
\item The \emph{HTML} document, that combines both textual output and figures.
\end{enumerate}

\section{Example}
This is an embedded R code example. All input files of this example are located in the subdirectory \emph{doc} of the installed \emph{GWAtoolbox} package.

Consider the two GWAS data files: \emph{gwa\_data\_example\_1.tbl} and \emph{gwa\_data\_example\_2.csv}. 
The first file contains 16 columns separated with tabulation:
<<>>=
t <- read.table("gwa_data_example_1.tbl", header = T, nrow = 1, sep = "\t")
colnames(t)
@
At the same time, the second file also contains 16 columns, however separated with comma:
<<>>=
t <- read.table("gwa_data_example_2.csv", header = T, nrow = 1, sep = ",")
colnames(t)
@
In order to perform the quality control check of these two files with \emph{GWAtoolbox} package, we prepare a simple input script \emph{GWAS\_script.txt}. Below are listed commands, which were inlcuded in the script:
<<>>=
cat(readLines("GWASQC_script.txt"), sep = "\n")
@
When the input script was prepared, we load the \emph{GWAtoolbox} library and call the \emph{gwasqc()} function as follows: 
<<results=hide>>=
library(GWAtoolbox)
gwasqc("GWASQC_script.txt")
@
As a result, the following output files were generated:
<<echo=FALSE>>=
cat(list.files(pattern="^(res.*)|(.*(html|png))"), sep="\n")
@

\section{Additional Tools}
\emph{Coming soon}.

\begin{thebibliography}{9}

\bibitem{Willer10}
Cristen J. Willer , Yun Li , and Gon�alo R. Abecasis. (2010) \textbf{METAL: fast and efficient meta-analysis of genomewide association scans}. Bioinformatics 26: 2190-2191.

\bibitem{Bakker08}
Paul I.W. de Bakker , Manuel A.R. Ferreira , Xiaoming Jia , Benjamin M. Neale , Soumya Raychaudhuri , and Benjamin F. Voight (2008) \textbf{Practical aspects of imputation-driven meta-analysis of genome-wide association studies}. Hum. Mol. Genet. 17: R122-R128.

\end{thebibliography}

\clearpage
\phantomsection
\addcontentsline{toc}{section}{Index}
\printindex

\end{document}
